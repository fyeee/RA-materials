{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_indu_index_mapping(df):\n",
    "    \"\"\"\n",
    "    Construct a dictionary with\n",
    "    key: industry code\n",
    "    value: indexes of all reports in the dataframe\n",
    "    \"\"\"\n",
    "    industries_to_index = {}\n",
    "    industries = df[\"ggroup\"].dropna().astype(int).unique()\n",
    "    industries = industries.tolist()\n",
    "    quarters = (df[\"year\"].astype(\"str\") + \" q\" + df[\"quarter\"].astype(\"str\")).unique()\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i, :]\n",
    "        if math.isnan(row[\"ggroup\"]):\n",
    "            continue\n",
    "        industries_to_index[int(row[\"ggroup\"])] = industries_to_index.get(int(row[\"ggroup\"]), set())\n",
    "        industries_to_index[int(row[\"ggroup\"])].add(i)\n",
    "    return industries_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_quar_index_mapping(df):\n",
    "    \"\"\"\n",
    "    Construct a dictionary with\n",
    "    key: quarter\n",
    "    value: indexes of all reports in the dataframe\n",
    "    \"\"\"\n",
    "    quarters = (df[\"year\"].astype(\"str\") + \" q\" + df[\"quarter\"].astype(\"str\")).unique()\n",
    "    quarter_to_index = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i, :]\n",
    "        quarter = row[\"year\"].astype(\"str\") + \" q\" + row[\"quarter\"].astype(\"str\")\n",
    "        quarter_to_index[quarter] = quarter_to_index.get(quarter, set())\n",
    "        quarter_to_index[quarter].add(i)\n",
    "    return quarter_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_analyst_index_mapping(df, all_files_dcns):\n",
    "    \"\"\"\n",
    "    Construct a dictionary with\n",
    "    key: analyst\n",
    "    value: indexes of all reports in the dataframe with the given DCNs(unique identification code for the reports)\n",
    "    \"\"\"\n",
    "    analyst_to_index = {}\n",
    "    for i, (_, dcn) in enumerate(all_files_dcns):\n",
    "        analyst = max(df[df[\"DCN\"] == dcn][\"Analyst\"])\n",
    "        if not analyst is np.nan:\n",
    "            analyst_to_index[analyst] = analyst_to_index.get(analyst, []) + [i]\n",
    "    return analyst_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_companies(df, indexes):\n",
    "    \"\"\"\n",
    "    Return the set of companies in the dataframe with the given indexes\n",
    "    \"\"\"\n",
    "    raw_companies = df.iloc[list(s), 4].unique()\n",
    "    all_companies = set()\n",
    "    for item in raw_companies:\n",
    "        l = item.split(\",\")\n",
    "        for company in l:\n",
    "            all_companies.add(company.strip(\" \").strip(\"^L19\"))\n",
    "    return all_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_files(target_dcns, company):\n",
    "    \"\"\"\n",
    "    Return a list of tuples that contains file paths and DCNs of all reports with the target DCNs\n",
    "    \"\"\"\n",
    "    directory = r\".\\PDFParsing\\parsed_files\"\n",
    "    files = []\n",
    "    temp = os.path.join(directory, company)\n",
    "    list_files = os.listdir(temp)\n",
    "    for item in list_files:\n",
    "        l = item.split(\"-\")\n",
    "        dcn = l[-1].rstrip(\".txt\")\n",
    "        while dcn and not dcn[-1].isdigit():\n",
    "            dcn = dcn[:-1]\n",
    "        while dcn and not dcn[0].isdigit():\n",
    "            dcn = dcn[1:]\n",
    "        if dcn:\n",
    "            dcn = int(dcn)\n",
    "        else:\n",
    "            continue\n",
    "        if dcn in target_dcns:\n",
    "            files.append((os.path.join(temp, item), dcn))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> This is an example run on industry code 2030 and Q4 of 2018. We will first find all reports with industry code 2030 and Q4 of 2018 and group them by analyst</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"metadata_reports_noduplicates_with_industry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries_to_index = construct_indu_index_mapping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_to_index = construct_quar_index_mapping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_to_index = construct_analyst_index_mapping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have a list of indexes of reports with industry code 2030 and Q4 of 2018\n",
    "indexes = industries_to_index[2030].intersection(quarter_to_index[\"2018 q4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCN is the unique identification code for the reports\n",
    "dcns = set(df.iloc[list(indexes), :][\"DCN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_companies = get_all_companies(df, indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAL.OQ',\n",
       " 'AAWW.OQ',\n",
       " 'ALK.N',\n",
       " 'ARCB.OQ',\n",
       " 'AZUL.N',\n",
       " 'CHRW.OQ',\n",
       " 'CNR.TO',\n",
       " 'CP.TO',\n",
       " 'CSX.OQ',\n",
       " 'CYRX.OQ',\n",
       " 'DAL.N',\n",
       " 'DSKE.OQ',\n",
       " 'ECHO.OQ',\n",
       " 'EXPD.OQ',\n",
       " 'FDX.N',\n",
       " 'GWR.N',\n",
       " 'HAG.DE',\n",
       " 'HTLD.OQ',\n",
       " 'HUBG.OQ',\n",
       " 'JBHT.OQ',\n",
       " 'JBLU.OQ',\n",
       " 'KNX.N',\n",
       " 'KSU.N',\n",
       " 'MESA.OQ',\n",
       " 'MSFT.OQ',\n",
       " 'NEO.OQ',\n",
       " 'NEO.TO',\n",
       " 'NSC.N',\n",
       " 'ODFL.OQ',\n",
       " 'ORBC.OQ',\n",
       " 'RLGT.A',\n",
       " 'SAIA.OQ',\n",
       " 'UAL.OQ',\n",
       " 'UNP.N',\n",
       " 'UPS.N',\n",
       " 'UV.N',\n",
       " 'VSAT.OQ',\n",
       " 'WERN.OQ',\n",
       " 'XPO.N',\n",
       " 'YRCW.OQ'}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_dcns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to excessive time needed to transfer files from Compute Canada to local disk, I only use subset of companies for this example\n",
    "for companies in [\"AAL.OQ\", 'ALK.N', 'FDX.N', \"DAL.N\", \"UAL.OQ\"]:\n",
    "    all_files_dcns += get_company_files(dcns, companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_to_index = construct_analyst_index_mapping(df, all_files_dcns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Now we have got all information needed, we will run LDA with corpus of all the files in all_files_dcns list and we will get a topic distribution for reports each analyst covered </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "from gensim import corpora, models, similarities\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "\n",
    "did=0\n",
    "for fname, _ in all_files_dcns:\n",
    "    f = open(fname, 'r')\n",
    "    result = f.read()\n",
    "    tokens = word_tokenize(result)\n",
    "    tokens = list(filter((\"--\").__ne__, tokens))\n",
    "    tokens = list(filter((\"fy\").__ne__, tokens))\n",
    "    words.append(tokens)\n",
    "    did+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.6 s\n"
     ]
    }
   ],
   "source": [
    "dictionary_LDA = corpora.Dictionary(words)\n",
    "#dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in words]\n",
    "\n",
    "num_topics = 8\n",
    "%time lda_model = models.LdaMulticore(corpus=corpus,\\\n",
    "                                        id2word=dictionary_LDA,\\\n",
    "                                        num_topics=num_topics, \\\n",
    "                                        random_state=100,\\\n",
    "                                        chunksize=10,\\\n",
    "                                        passes=10,\\\n",
    "                                        alpha=0.3,\\\n",
    "                                        eta=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Credit Suisse Global Product Marketing': [0, 1, 2, 3, 165],\n",
       " 'Mr. Frederic Bastien, CFA': [4, 34],\n",
       " 'Ms. Helane R. Becker': [5,\n",
       "  14,\n",
       "  32,\n",
       "  35,\n",
       "  44,\n",
       "  62,\n",
       "  64,\n",
       "  67,\n",
       "  69,\n",
       "  85,\n",
       "  92,\n",
       "  93,\n",
       "  98,\n",
       "  101,\n",
       "  106,\n",
       "  113,\n",
       "  127,\n",
       "  131,\n",
       "  140,\n",
       "  154,\n",
       "  157,\n",
       "  160],\n",
       " 'Duane T. Pfennigwerth, CFA': [6, 17, 36, 47, 65, 73, 155],\n",
       " 'Danny Goode': [7,\n",
       "  8,\n",
       "  19,\n",
       "  20,\n",
       "  33,\n",
       "  37,\n",
       "  38,\n",
       "  49,\n",
       "  50,\n",
       "  63,\n",
       "  129,\n",
       "  135,\n",
       "  141,\n",
       "  163,\n",
       "  166,\n",
       "  170],\n",
       " 'Hunter K. Keay': [9, 39, 79, 89, 94, 133, 153, 164],\n",
       " 'Mr. Michael W. Derchin': [10, 31, 40, 61, 66, 90, 128, 138, 150, 158],\n",
       " 'Michael J. Linenberg': [11, 16, 41, 46, 72, 87, 134, 139, 161],\n",
       " 'Andrew G. Didora, CFA': [12, 42, 70, 84, 126, 151, 169],\n",
       " 'Mr. James A. Corridore': [13,\n",
       "  15,\n",
       "  43,\n",
       "  45,\n",
       "  68,\n",
       "  71,\n",
       "  75,\n",
       "  104,\n",
       "  109,\n",
       "  112,\n",
       "  130,\n",
       "  137,\n",
       "  144,\n",
       "  159],\n",
       " 'Adam Hackel': [18, 48, 162],\n",
       " 'Mr. Jack L. Atkins': [21, 51, 102, 118, 132, 167],\n",
       " 'Daniel J. McKenzie, CFA': [22, 52, 74, 91, 136, 152],\n",
       " 'Mr. Christopher D. Quilty': [23, 53],\n",
       " 'Mr. Jose Caiado De Sousa': [24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  77,\n",
       "  78,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  86,\n",
       "  142,\n",
       "  143,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176],\n",
       " 'Mr. Christopher N. Stathoulopoulos': [30, 60, 149, 177],\n",
       " 'SADIF Research': [76],\n",
       " 'Rajeev Lalwani': [88, 156],\n",
       " 'Mr. Keith Schoonmaker': [95, 97, 99, 117, 121, 123, 124],\n",
       " 'Mr. Amit Mehrotra': [96, 100, 115],\n",
       " 'Mr. Brandon R. Oglenski': [103, 110],\n",
       " 'Mr. Ken S. Hoexter': [105, 111],\n",
       " 'Ms. Allison M. Landry': [107, 114],\n",
       " 'William Fitzalan Howard': [108],\n",
       " 'Ravi Shanker': [116, 120],\n",
       " 'Mr. Scott H. Group': [119],\n",
       " 'Bascome Majors': [122],\n",
       " 'Analyst Desk TeleTrade': [125],\n",
       " 'Mr Neil Glynn, CFA': [168]}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "for analyst, indexes in analyst_to_index.items():\n",
    "    row = [0] * num_topics\n",
    "    all_words = []\n",
    "    for i in indexes:\n",
    "        all_words.extend(words[i])\n",
    "    topics = lda_model.get_document_topics(dictionary_LDA.doc2bow(all_words), minimum_probability = 1e-4)\n",
    "    for index, dist in topics:\n",
    "        row[index] = dist\n",
    "    matrix.append(row)\n",
    "matrix = np.array(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Calculating the shapley values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Powerset as ps\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_rows(x: np.ndarray): # function to normalize rows in a two-dimensional materix\n",
    "    return x/np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "# shapley values function\n",
    "def shapley_values(loading_matrix):\n",
    "    \n",
    "    loading_matrix=normalize_rows(loading_matrix)\n",
    "\n",
    "    no_analysts=np.shape(np.dot(loading_matrix,loading_matrix.T))[1] # number of analysts\n",
    "    list_analysts=[x for x in range(no_analysts)]\n",
    "    data=pd.DataFrame(columns={'Analyst','InfoContribution'})\n",
    "\n",
    "    for k in range(no_analysts):\n",
    "        list_minusone=[x for x in list_analysts if x!=k] # list without the analyst\n",
    "        all_sets=[x for x in ps.powerset(list_minusone) if x]\n",
    "\n",
    "        shapley_value=[]\n",
    "\n",
    "        for coalition in all_sets:\n",
    "            \n",
    "            other_coal=loading_matrix[coalition,:].sum(axis=0)\n",
    "            other_coal=other_coal/np.linalg.norm(other_coal,ord=2,axis=0,keepdims=True)\n",
    "            \n",
    "            contribution=1-np.dot(other_coal,loading_matrix[k,:])\n",
    "\n",
    "            shapley_value.append(contribution)\n",
    "\n",
    "            #print(coalition, np.dot(other_coal,loading_matrix[k,:]), contribution)\n",
    "\n",
    "        #print(np.array(shapley_value).mean())\n",
    "        data=data.append({'Analyst': k,'InfoContribution': np.array(shapley_value).mean()},ignore_index=True)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# get informational diversity measure\n",
    "def diversity(loading_matrix):\n",
    "    ld_matrix_norm=normalize_rows(loading_matrix) # normalize all row vectors\n",
    "    cosine_matrix=np.dot(ld_matrix_norm,ld_matrix_norm.T) # compute dot products across normalized rows\n",
    "    avg_similarity=cosine_matrix[np.triu_indices(np.shape(cosine_matrix)[1],k=1)].mean()\n",
    "    \n",
    "    if np.shape(loading_matrix)[0]==1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1-avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.99864340e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.08690758e-04, 9.99707639e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.59938391e-02,\n",
       "        9.78265762e-01, 0.00000000e+00, 0.00000000e+00, 5.67171536e-03],\n",
       "       [1.81101466e-04, 2.49074027e-01, 1.95501678e-04, 2.63312645e-03,\n",
       "        4.71199840e-01, 5.83241973e-03, 3.16843987e-02, 2.39199623e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.99956667e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 7.84895062e-01, 2.11509666e-03, 9.79568064e-03,\n",
       "        3.11307702e-02, 1.25849947e-01, 0.00000000e+00, 4.60288264e-02],\n",
       "       [0.00000000e+00, 3.21828248e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.92820442e-01, 0.00000000e+00, 0.00000000e+00, 3.71632888e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.99842584e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 9.67780113e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.00821047e-02, 0.00000000e+00, 0.00000000e+00, 1.70396292e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.99830067e-01, 0.00000000e+00, 0.00000000e+00, 1.09279281e-04],\n",
       "       [1.59596806e-04, 1.80382238e-04, 1.77634254e-04, 1.74378190e-04,\n",
       "        9.96370554e-01, 1.68574858e-04, 1.59651856e-04, 2.60922150e-03],\n",
       "       [3.65572865e-04, 4.28442955e-01, 4.00995312e-04, 4.05674393e-04,\n",
       "        5.65840065e-01, 3.77616286e-03, 3.66359425e-04, 4.02214180e-04],\n",
       "       [0.00000000e+00, 7.75177360e-01, 1.09446002e-04, 1.21390121e-02,\n",
       "        1.89727649e-01, 1.14539958e-04, 0.00000000e+00, 2.25323066e-02],\n",
       "       [0.00000000e+00, 1.00694189e-04, 1.01319063e-04, 5.03861764e-03,\n",
       "        1.01440346e-04, 9.76321280e-01, 0.00000000e+00, 1.81523748e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 7.92434067e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.20697510e-01],\n",
       "       [0.00000000e+00, 9.99895334e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.62647653e-04, 2.24541202e-02, 6.84152357e-04, 3.64836119e-02,\n",
       "        9.32822824e-01, 5.52876480e-03, 6.63289684e-04, 7.00613949e-04],\n",
       "       [2.48934579e-04, 7.24653423e-01, 2.75965198e-04, 2.32672505e-02,\n",
       "        6.56392425e-02, 3.28150287e-04, 2.49052973e-04, 1.85338050e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.99882162e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.99445856e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.83154727e-04, 1.20912842e-01, 1.97733156e-04, 3.85035612e-02,\n",
       "        2.91773617e-01, 3.83104801e-01, 1.83144410e-04, 1.65141150e-01],\n",
       "       [1.48448918e-04, 8.84073198e-01, 1.57874398e-04, 1.66627346e-04,\n",
       "        1.14980817e-01, 1.63524586e-04, 1.48545674e-04, 1.60968062e-04],\n",
       "       [2.33466795e-04, 6.58329368e-01, 1.99589040e-03, 1.46723501e-02,\n",
       "        9.91460532e-02, 9.94955599e-02, 2.33777959e-04, 1.25893489e-01],\n",
       "       [2.34247535e-04, 7.01332688e-01, 2.56908737e-04, 4.29686606e-02,\n",
       "        1.24009199e-01, 7.11562335e-02, 2.34489693e-04, 5.98075464e-02],\n",
       "       [2.15716907e-04, 8.30750108e-01, 2.36459207e-04, 7.98365392e-04,\n",
       "        1.20818503e-02, 1.55342564e-01, 2.15865512e-04, 3.59104917e-04],\n",
       "       [2.95742357e-04, 4.12445754e-01, 3.29790841e-04, 7.32567683e-02,\n",
       "        8.17686841e-02, 4.31275994e-01, 2.96287850e-04, 3.31023446e-04],\n",
       "       [3.34675540e-04, 7.80847251e-01, 3.55971075e-04, 4.57876042e-04,\n",
       "        2.16942921e-01, 3.65974993e-04, 3.34561279e-04, 3.60815699e-04],\n",
       "       [6.34350406e-04, 1.24278642e-01, 6.79717399e-04, 6.78310695e-04,\n",
       "        7.26638198e-01, 1.45766199e-01, 6.34744240e-04, 6.89825742e-04],\n",
       "       [4.46260528e-04, 6.34372115e-01, 6.63528044e-04, 4.65228707e-02,\n",
       "        1.19067416e-01, 4.76229499e-04, 4.45976737e-04, 1.98005632e-01]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-6c894c493da2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshapley_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-258-1f3dbe8b7cbb>\u001b[0m in \u001b[0;36mshapley_values\u001b[1;34m(loading_matrix)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_analysts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlist_minusone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_analysts\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# list without the analyst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mall_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpowerset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_minusone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mshapley_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\Powerset\\__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Powerset of a List\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m powerset = lambda s: sorted([[s[j] for j in range(len(s)) if (i & (1 << j))] for i in range(1 << len(s))]\n\u001b[0m\u001b[0;32m      3\u001b[0m                             , key = lambda y: len(y))\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Lengths of the sublists (subsets), cardinality of the subsets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\Powerset\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Powerset of a List\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m powerset = lambda s: sorted([[s[j] for j in range(len(s)) if (i & (1 << j))] for i in range(1 << len(s))]\n\u001b[0m\u001b[0;32m      3\u001b[0m                             , key = lambda y: len(y))\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Lengths of the sublists (subsets), cardinality of the subsets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shapley_values(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
